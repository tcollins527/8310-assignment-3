{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import struct\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "import io"
      ],
      "metadata": {
        "id": "ZoVgfA4FgKB6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU if available, else CPU, to optimize training speed\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load a pre-trained model from a specified path\n",
        "model = torch.load('/model.pth')"
      ],
      "metadata": {
        "id": "eULaDn2BMk4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read image data\n",
        "def read_images(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
        "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "        images = images.reshape(num_images, 1, rows, cols)\n",
        "        images = images.astype(np.float32) / 255.0\n",
        "    return images\n",
        "\n",
        "# Function to read label data\n",
        "def read_labels(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
        "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "        labels = labels.astype(np.int64)\n",
        "    return labels"
      ],
      "metadata": {
        "id": "QsbQfz63MmwO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths for training and testing data\n",
        "train_images_file = 'FashionMNIST/raw/train-images-idx3-ubyte'\n",
        "train_labels_file = 'FashionMNIST/raw/train-labels-idx1-ubyte'\n",
        "test_images_file = 'FashionMNIST/raw/t10k-images-idx3-ubyte'\n",
        "test_labels_file = 'FashionMNIST/raw/t10k-labels-idx1-ubyte'\n",
        "\n",
        "# Load the training & testing data\n",
        "train_images = read_images(train_images_file)\n",
        "train_labels = read_labels(train_labels_file)\n",
        "test_images = read_images(test_images_file)\n",
        "test_labels = read_labels(test_labels_file)"
      ],
      "metadata": {
        "id": "wtDN8UNnMn1v"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMNIST(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "qyIP8sXyMpFR"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom DataLoader to handle batch loading\n",
        "class CustomDataLoader:\n",
        "    def __init__(self, dataset, batch_size=64, shuffle=True):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.num_samples = len(dataset)\n",
        "        self.indices = np.arange(self.num_samples)\n",
        "        self.current_idx = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.current_idx = 0\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.current_idx >= self.num_samples:\n",
        "            raise StopIteration\n",
        "\n",
        "        batch_indices = self.indices[self.current_idx:self.current_idx + self.batch_size]\n",
        "        batch = [self.dataset[idx] for idx in batch_indices]\n",
        "\n",
        "        batch_images, batch_labels = zip(*batch)\n",
        "\n",
        "        batch_images = torch.stack(batch_images)\n",
        "        batch_labels = torch.stack(batch_labels)\n",
        "\n",
        "        self.current_idx += self.batch_size\n",
        "\n",
        "        return batch_images, batch_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.num_samples + self.batch_size - 1) // self.batch_size"
      ],
      "metadata": {
        "id": "phOg6ge9R2Vw"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize datasets --> loaders\n",
        "train_dataset = CustomMNIST(train_images, train_labels)\n",
        "test_dataset = CustomMNIST(test_images, test_labels)\n",
        "train_dataloader = CustomDataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = CustomDataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "6tnD9yaMP3XX"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, use_1x1conv=False, strides=1):\n",
        "        super(Residual, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=3, padding=1, stride=strides)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2d(\n",
        "                in_channels, out_channels, kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)"
      ],
      "metadata": {
        "id": "lj-e3LjxMq0V"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, arch, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            1, self.in_channels, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(\n",
        "            kernel_size=3, stride=2, padding=1)\n",
        "        self.layers = self._make_layers(arch)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(self.in_channels, num_classes)\n",
        "\n",
        "    def _make_layers(self, arch):\n",
        "        layers = []\n",
        "        for num_blocks, out_channels in arch:\n",
        "            strides = 1 if len(layers) == 0 else 2\n",
        "            layers.append(\n",
        "                self._make_layer(out_channels, num_blocks, strides))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, strides):\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            Residual(self.in_channels, out_channels, use_1x1conv=True, strides=strides))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(\n",
        "                Residual(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layers(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class ResNet18(ResNet):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet18, self).__init__(\n",
        "            arch=((2, 64), (2, 128), (2, 256), (2, 512)),\n",
        "            num_classes=num_classes)"
      ],
      "metadata": {
        "id": "d3M_3XhAMsJd"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model on the training dataset\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    print_interval = max(1, num_batches // 5)\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % print_interval == 0:\n",
        "            loss_value = loss.item()\n",
        "            current = batch_idx * len(X)\n",
        "            print(f\"loss: {loss_value:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# Function to evaluate the model on the test dataset\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error:\\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n",
        "\n",
        "# Function to train the entire network with multiple epochs\n",
        "def train_net(model, train_dataloader, test_dataloader, epochs=20, learning_rate=1e-3):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for t in range(epochs):\n",
        "        try:\n",
        "            print(f\"Epoch {model.EPOCH + t + 1}\\n-------------------------------\")\n",
        "        except AttributeError:\n",
        "            print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "        train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "        test_loop(test_dataloader, model, loss_fn)\n",
        "    print(\"Done!\")\n",
        "\n",
        "    try:\n",
        "        model.EPOCH += epochs\n",
        "    except AttributeError:\n",
        "        model.EPOCH = epochs\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "zSe8iqvIMvnW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = train_net(model, train_dataloader, test_dataloader, epochs=5, learning_rate=1e-3)"
      ],
      "metadata": {
        "id": "E5YZwfM-MwIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model's state to a file\n",
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "os4cze9GNxHr"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nl8g5Gc7QW2K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}