{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import struct\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "import io"
      ],
      "metadata": {
        "id": "ZoVgfA4FgKB6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU if available, else CPU, to optimize training speed\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load a pre-trained model from a specified path\n",
        "model = torch.load('/model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eULaDn2BMk4K",
        "outputId": "cbf79f07-3d9c-4f0a-b5df-c13dbd24c319"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-9920ce5842e1>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load('/content/model.pth')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read image data\n",
        "def read_images(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
        "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "        images = images.reshape(num_images, 1, rows, cols)\n",
        "        images = images.astype(np.float32) / 255.0\n",
        "    return images\n",
        "\n",
        "# Function to read label data\n",
        "def read_labels(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
        "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "        labels = labels.astype(np.int64)\n",
        "    return labels"
      ],
      "metadata": {
        "id": "QsbQfz63MmwO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths for training and testing data\n",
        "train_images_file = 'FashionMNIST/raw/train-images-idx3-ubyte'\n",
        "train_labels_file = 'FashionMNIST/raw/train-labels-idx1-ubyte'\n",
        "test_images_file = 'FashionMNIST/raw/t10k-images-idx3-ubyte'\n",
        "test_labels_file = 'FashionMNIST/raw/t10k-labels-idx1-ubyte'\n",
        "\n",
        "# Load the training & testing data\n",
        "train_images = read_images(train_images_file)\n",
        "train_labels = read_labels(train_labels_file)\n",
        "test_images = read_images(test_images_file)\n",
        "test_labels = read_labels(test_labels_file)"
      ],
      "metadata": {
        "id": "wtDN8UNnMn1v"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMNIST(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "qyIP8sXyMpFR"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom DataLoader to handle batch loading\n",
        "class CustomDataLoader:\n",
        "    def __init__(self, dataset, batch_size=64, shuffle=True):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.num_samples = len(dataset)\n",
        "        self.indices = np.arange(self.num_samples)\n",
        "        self.current_idx = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.current_idx = 0\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.current_idx >= self.num_samples:\n",
        "            raise StopIteration\n",
        "\n",
        "        batch_indices = self.indices[self.current_idx:self.current_idx + self.batch_size]\n",
        "        batch = [self.dataset[idx] for idx in batch_indices]\n",
        "\n",
        "        batch_images, batch_labels = zip(*batch)\n",
        "\n",
        "        batch_images = torch.stack(batch_images)\n",
        "        batch_labels = torch.stack(batch_labels)\n",
        "\n",
        "        self.current_idx += self.batch_size\n",
        "\n",
        "        return batch_images, batch_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.num_samples + self.batch_size - 1) // self.batch_size"
      ],
      "metadata": {
        "id": "phOg6ge9R2Vw"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize datasets --> loaders\n",
        "train_dataset = CustomMNIST(train_images, train_labels)\n",
        "test_dataset = CustomMNIST(test_images, test_labels)\n",
        "train_dataloader = CustomDataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = CustomDataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "6tnD9yaMP3XX"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, use_1x1conv=False, strides=1):\n",
        "        super(Residual, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=3, padding=1, stride=strides)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2d(\n",
        "                in_channels, out_channels, kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)"
      ],
      "metadata": {
        "id": "lj-e3LjxMq0V"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, arch, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            1, self.in_channels, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(\n",
        "            kernel_size=3, stride=2, padding=1)\n",
        "        self.layers = self._make_layers(arch)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(self.in_channels, num_classes)\n",
        "\n",
        "    def _make_layers(self, arch):\n",
        "        layers = []\n",
        "        for num_blocks, out_channels in arch:\n",
        "            strides = 1 if len(layers) == 0 else 2\n",
        "            layers.append(\n",
        "                self._make_layer(out_channels, num_blocks, strides))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, strides):\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            Residual(self.in_channels, out_channels, use_1x1conv=True, strides=strides))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(\n",
        "                Residual(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layers(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class ResNet18(ResNet):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet18, self).__init__(\n",
        "            arch=((2, 64), (2, 128), (2, 256), (2, 512)),\n",
        "            num_classes=num_classes)"
      ],
      "metadata": {
        "id": "d3M_3XhAMsJd"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model on the training dataset\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    print_interval = max(1, num_batches // 5)\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % print_interval == 0:\n",
        "            loss_value = loss.item()\n",
        "            current = batch_idx * len(X)\n",
        "            print(f\"loss: {loss_value:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# Function to evaluate the model on the test dataset\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error:\\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n",
        "\n",
        "# Function to train the entire network with multiple epochs\n",
        "def train_net(model, train_dataloader, test_dataloader, epochs=20, learning_rate=1e-3):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for t in range(epochs):\n",
        "        try:\n",
        "            print(f\"Epoch {model.EPOCH + t + 1}\\n-------------------------------\")\n",
        "        except AttributeError:\n",
        "            print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "        train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "        test_loop(test_dataloader, model, loss_fn)\n",
        "    print(\"Done!\")\n",
        "\n",
        "    try:\n",
        "        model.EPOCH += epochs\n",
        "    except AttributeError:\n",
        "        model.EPOCH = epochs\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "zSe8iqvIMvnW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = train_net(model, train_dataloader, test_dataloader, epochs=5, learning_rate=1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "E5YZwfM-MwIk",
        "outputId": "586b8fc2-4dfa-4fb5-cf34-c9299610c015"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.006928  [    0/60000]\n",
            "loss: 0.009806  [11968/60000]\n",
            "loss: 0.001002  [23936/60000]\n",
            "loss: 0.002928  [35904/60000]\n",
            "loss: 0.000852  [47872/60000]\n",
            "loss: 0.000340  [59840/60000]\n",
            "Test Error:\n",
            " Accuracy: 89.1%, Avg loss: 0.621959\n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.008950  [    0/60000]\n",
            "loss: 0.000301  [11968/60000]\n",
            "loss: 0.000701  [23936/60000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-7964ef2ee5a1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-f9d5991002fc>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(model, train_dataloader, test_dataloader, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t + 1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-f9d5991002fc>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model's state to a file\n",
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "os4cze9GNxHr"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nl8g5Gc7QW2K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}